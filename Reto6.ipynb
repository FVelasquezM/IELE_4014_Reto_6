{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user numpy\n",
    "%pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClassifier:\n",
    "    \n",
    "    def __init__(self, hidden_layer, sensitivity, suppress_output = False):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.suppress_output = suppress_output\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.outputs = None\n",
    "        self.sensitivity = sensitivity\n",
    "    \n",
    "    def fit(self, X, y, max_iterations = None):\n",
    "        \n",
    "        if max_iterations == None: \n",
    "            max_iterations = 2*len(X)\n",
    "    \n",
    "        #Se hará la suposición que todas las filas de X tienen el mismo número de columnas.\n",
    "        self.build_network(len(X[0]))\n",
    "   \n",
    "        batch_size = 100\n",
    "        last_error = 1\n",
    "        no_improv = 0\n",
    "        for i in range(200*len(X)):\n",
    "            \n",
    "            batch_acum = 0\n",
    "            for j in range(batch_size):\n",
    "                index = math.floor(random.random()*len(X))\n",
    "                y_pred = self.feed_forward(X[index])\n",
    "                batch_acum+= self.logloss(y[index], y_pred)\n",
    "            \n",
    "            e = batch_acum/batch_size\n",
    "            print(\"e\")\n",
    "            print(e)\n",
    "            \n",
    "            self.back_propagation(e, learning_rate=0.1)\n",
    "            \n",
    "            #if(i%10000 == 0):\n",
    "            #    print(\"e\")\n",
    "            #    print(e)\n",
    "            \n",
    "            if last_error <= e:\n",
    "                no_improv += 1\n",
    "            else: \n",
    "                no_improv = 0\n",
    "                \n",
    "            if no_improv == 10*len(X):\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    #input_length esero entero positivo mayor a 1 que representa la cantidad de atributos de cada\n",
    "    #dato de entrada (sin incluir el que se busca predecir).\n",
    "    #hidden_dimensions es una lista que contiene las dimensiones de las capas escondidas de la red\n",
    "    #si hidden_dimensions es [3,3,3] se crearán tres capas escondidas de \n",
    "    def build_network(self, input_length):\n",
    "        \n",
    "        dimensions = self.hidden_layer                   \n",
    "        \n",
    "        #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "        #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "        W = []  \n",
    "        for i in range(len(dimensions)): \n",
    "            W.append([])\n",
    "        \n",
    "        \n",
    "        #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "        #de la capa 0\n",
    "        b = []\n",
    "        for i in range(len(dimensions)): \n",
    "            b.append([])\n",
    "        \n",
    "        \n",
    "        #todas las neuronas en la primera capa deben tener input_lenght pesos. De ahí en adelante, \n",
    "        #cada neurona en la capa j tendrá len(W[j-1]) pesos.\n",
    "        layer_width = dimensions[0]\n",
    "        \n",
    "        for i in range(layer_width):\n",
    "            W[0].append([])\n",
    "            W[0][i] = [random.uniform(-1,1) for j in range(input_length)]\n",
    "            b[0].append(0)\n",
    "            b[0][i] = random.uniform(-1,1)\n",
    "            W[0][i] = np.array([W[0][i]])\n",
    "            \n",
    "\n",
    "            \n",
    "        #Inicialización aleatoria de pesos y bias entre -1 y 1\n",
    "        for i in range(1, len(dimensions)):\n",
    "            layer_width = dimensions[i]\n",
    "            for j in range(layer_width):\n",
    "                #se inicializan n valores aleatorios entre -1 y 1 donde n = len(W[i-1])\n",
    "                W[i].append([])\n",
    "                W[i][j] = [random.uniform(-1,1) for j in range(len(W[i-1]))]\n",
    "                b[i].append(0)\n",
    "                #se crea un bias aleatorio para la neurona\n",
    "                b[i][j] = random.uniform(-1,1) \n",
    "            \n",
    "                W[i][j] = np.array([W[i][j]])\n",
    "                \n",
    "        \n",
    "        #Ahora se insertan los pesos y bias al output unit\n",
    "        W.append([])\n",
    "        W[len(W) - 1].append([])\n",
    "        b.append([])\n",
    "        b[len(W) - 1].append(0)\n",
    "        W[len(W) - 1][0] = [random.uniform(-1,1) for j in range(len(W[len(W)-2]))]\n",
    "        W[len(W) - 1][0] = np.array([W[len(W) - 1][0]])\n",
    "        b[len(W) - 1][0] = random.uniform(-1,1)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"Network Built\")\n",
    "            print(\"W\")\n",
    "            print(W)\n",
    "            print(\"b\")\n",
    "            print(b)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "            \n",
    "    #X es una entrada\n",
    "    #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "    #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "    #l será el número de capas.\n",
    "    #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "    #de la capa 0.\n",
    "    #Algoritmo se basa en el algoritmo 6.3 de GoodFellow et al \n",
    "    #y en el algoritmo de la página 217 de Grus en Data Science from Scratch\n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        l = len(self.hidden_layer)\n",
    "        \n",
    "        #Salidas de cada neurona. Primera dimensión capa, segunda neuronas\n",
    "        #outputs[0][0] representa la entrada 0 de X. \n",
    "        #outputs[1][0] representa la salida 0 de la primera capa escondida.\n",
    "        outputs = [np.array([x])]\n",
    "        A = []\n",
    "        \n",
    "        for layer in range(1,l+1):\n",
    "            layer_out = []\n",
    "            A_out = []\n",
    "            for neuron in range(len(self.W[layer-1])):\n",
    "                #a es el resultado de las salidas de la capa anterior por los pesos de la capa actual.\n",
    "                \n",
    "                X = outputs[layer-1].transpose()\n",
    "                W = self.W[layer-1][neuron]\n",
    "                a = np.dot(W,X)[0][0] + self.b[layer-1][neuron]\n",
    "                layer_out.append(self.relu(a))\n",
    "                \n",
    "            outputs.append(np.array([layer_out]))\n",
    "            \n",
    "        #Ahora se computa el output del output layer.\n",
    "        X = outputs[len(outputs)-1].transpose()\n",
    "        W = self.W[len(self.W)-1][0]       \n",
    "        a = np.dot(W,X)[0][0] + self.b[len(self.W)-1][0]\n",
    "        outputs.append(self.sigmoid(a))\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"----------------------Feed_forward outputs--------------------------\")\n",
    "            print(outputs)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        #Retorna el sigmoide del último output. Esto representa la probabilidad de que \n",
    "        #x pertenezca a la clase 1.\n",
    "        self.outputs = outputs\n",
    "        self.A = A\n",
    "        return outputs[len(outputs)-1]\n",
    "    \n",
    "    #Relu para hidden units\n",
    "    def relu(self, z):\n",
    "        return max(0,z)\n",
    "    \n",
    "    #Sigmoide para Output unit\n",
    "    def sigmoid(self, x):\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1+math.exp(x))\n",
    "        else:\n",
    "            return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def relu_derivative(self, z): \n",
    "        if z > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s*(1-s)\n",
    "    \n",
    "    #sea gradiente de la salida.\n",
    "    def back_propagation(self, g, learning_rate):\n",
    "        \n",
    "        W = self.W\n",
    " \n",
    "        #Calcular pesos de la capa de salida, se hace por aparte porque \n",
    "        #Usa función de activación sigmoide en lugar de relu.\n",
    "    \n",
    "        a = self.outputs[len(self.outputs)-1]\n",
    "        \n",
    "        f_a_prime = self.sigmoid_derivative(a)\n",
    "        \n",
    "        g = g*f_a_prime\n",
    "        \n",
    "        h = self.outputs[len(self.outputs)-2]\n",
    "        \n",
    "        w_grad = np.dot(g, h)\n",
    "        b_grad = g\n",
    "        \n",
    "        W_k  = W[len(W) - 1][0]\n",
    "        \n",
    "        g = np.dot(W_k.transpose(), g)\n",
    "            \n",
    "        #Ahora, ajustar W y b con w_grad y b_grad\n",
    "        W_k = W_k - np.dot(learning_rate,w_grad)\n",
    "           \n",
    "        W_k = self.matrix_to_listmatrix(W_k)\n",
    "        \n",
    "        W[len(W)-1] = W_k\n",
    "        \n",
    "        b = self.b[len(W) - 1]\n",
    "        b = b - np.dot(learning_rate,b_grad)\n",
    "        self.b[len(self.b)-1] = [b[0]]\n",
    "        for i in range(len(self.outputs)-2, 0, -1):\n",
    "            \n",
    "            a = self.outputs[i][0]\n",
    "            \n",
    "            f_a_prime = [self.sigmoid_derivative(j) for j in a]\n",
    "            f_a_prime = np.array([f_a_prime]).transpose()\n",
    "            \n",
    "            g = np.multiply(g, f_a_prime)\n",
    "            \n",
    "            h = self.outputs[i-1]\n",
    "        \n",
    "            w_grad = np.dot(g, h)\n",
    "            b_grad = g\n",
    "        \n",
    "            W_k  = W[i-1]\n",
    "            W_k = self.listmatrix_to_matrix(W_k)\n",
    "            g = np.dot(W_k.transpose(), g)\n",
    "            \n",
    "            #Ahora, ajustar W y b con w_grad y b_grad\n",
    "            W_k = W_k - np.dot(learning_rate,w_grad)\n",
    "            \n",
    "            W_k = self.matrix_to_listmatrix(W_k)\n",
    "            \n",
    "            W[i-1] = W_k\n",
    "            \n",
    "            b = self.b[i-1]\n",
    "            b = b - np.dot(learning_rate,b_grad)\n",
    "            self.b[i-1] = b[0]\n",
    "            \n",
    "        self.W = W\n",
    "    \n",
    "    def matrix_to_listmatrix(self, mat):\n",
    "        return [ np.array([i]) for i in mat]\n",
    "        \n",
    "    \n",
    "    def listmatrix_to_matrix(self, matlist):\n",
    "        return np.array([i[0] for i in matlist])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        y_pred = self.feed_forward(x)\n",
    "        \n",
    "        if y_pred >= self.sensitivity:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #Calcula el desempeño.\n",
    "    def accuracy(self, test_data_X, test_data_y):\n",
    "    \n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        \n",
    "        for i in range(0, len(test_data_X)):\n",
    "        \n",
    "            #si se predice 1 pero es 0\n",
    "            if  self.predict(test_data_X[i]) == 1 and test_data_y[i] == 0:\n",
    "                incorrect += 1\n",
    "            #si se predice 1 y es 1\n",
    "            elif self.predict(test_data_X[i]) == 1 and test_data_y[i] == 1:\n",
    "                correct+=1\n",
    "            #si se predice 0 y es 1\n",
    "            elif self.predict(test_data_X[i]) == 0 and test_data_y[i] == 1:\n",
    "                incorrect+=1\n",
    "            #si se predice 0 y es 0\n",
    "            else:\n",
    "                correct+=1\n",
    "                \n",
    "        return correct/(correct+incorrect)\n",
    "    \n",
    "    #para calcular el error de una calificación\n",
    "    def logloss(self, y, y_pred):\n",
    "        return -(y*math.log(y_pred+1e-15)+(1-y)*math.log(1-y_pred+1e-15)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 8330\n",
      "Columnas de la matriz: 31\n",
      "X shape(8330, 30)\n",
      "y shape(8330,)\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./msd_genre_dataset/fixed_ds.csv\", \"r\"), delimiter=\",\", skiprows=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "X = np.delete(data_matrix, len(data_matrix[0])-1,1)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "#Se intenta estandarizar X para lograr mejor desempeño. Sin embargo, no parece funcionar.\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#Los datos del set de datos están agrupados por género. Es decir, primero están todas las filas que corresponden\n",
    "#a 1 y después todas las que corresponden a -1. Se hace un shuffle para que, más tarde, en cross-validation\n",
    "#no se creen unos modelos que predigan únicamente una clase.\n",
    "np.random.shuffle(data_matrix)\n",
    "\n",
    "\n",
    "print(\"X shape\" + str(X.shape))\n",
    "print(\"y shape\" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "e\n",
      "0.6786930754983773\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fca07b131a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLayerPerceptronClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-1b3a97c39b2d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, max_iterations)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mbatch_acum\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_acum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-1b3a97c39b2d>\u001b[0m in \u001b[0;36mlogloss\u001b[0;34m(self, y, y_pred)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m#para calcular el error de una calificación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptronClassifier(hidden_layer = [3], suppress_output = True, sensitivity = 0.5)\n",
    "\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4815126050420168\n"
     ]
    }
   ],
   "source": [
    "print(mlp.accuracy(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
