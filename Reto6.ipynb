{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user numpy\n",
    "%pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClassifier:\n",
    "    \n",
    "    def __init__(self, hidden_layer, suppress_output = False):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.suppress_output = False\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.outputs = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        #Se hará la suposición que todas las filas de X tienen el mismo número de columnas.\n",
    "        self.build_network(len(X[0]))\n",
    "        \n",
    "        index = math.floor(random.random()*len(X))\n",
    "        \n",
    "        y_pred = self.feed_forward(X[index])\n",
    "        print(\"FF: %s \" % y_pred)\n",
    "        \n",
    "        self.back_propagation(0.23, learning_rate=0.01)\n",
    "\n",
    "    \n",
    "    #input_length esero entero positivo mayor a 1 que representa la cantidad de atributos de cada\n",
    "    #dato de entrada (sin incluir el que se busca predecir).\n",
    "    #hidden_dimensions es una lista que contiene las dimensiones de las capas escondidas de la red\n",
    "    #si hidden_dimensions es [3,3,3] se crearán tres capas escondidas de \n",
    "    def build_network(self, input_length):\n",
    "        \n",
    "        dimensions = self.hidden_layer                   \n",
    "        \n",
    "        #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "        #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "        W = []  \n",
    "        for i in range(len(dimensions)): \n",
    "            W.append([])\n",
    "        \n",
    "        \n",
    "        #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "        #de la capa 0\n",
    "        b = []\n",
    "        for i in range(len(dimensions)): \n",
    "            b.append([])\n",
    "        \n",
    "        \n",
    "        #todas las neuronas en la primera capa deben tener input_lenght pesos. De ahí en adelante, \n",
    "        #cada neurona en la capa j tendrá len(W[j-1]) pesos.\n",
    "        layer_width = dimensions[0]\n",
    "        \n",
    "        for i in range(layer_width):\n",
    "            W[0].append([])\n",
    "            W[0][i] = [random.uniform(-1,1) for j in range(input_length)]\n",
    "            b[0].append(0)\n",
    "            b[0][i] = random.uniform(-1,1)\n",
    "            W[0][i] = np.array([W[0][i]])\n",
    "            \n",
    "\n",
    "            \n",
    "        #Inicialización aleatoria de pesos y bias entre -1 y 1\n",
    "        for i in range(1, len(dimensions)):\n",
    "            layer_width = dimensions[i]\n",
    "            for j in range(layer_width):\n",
    "                #se inicializan n valores aleatorios entre -1 y 1 donde n = len(W[i-1])\n",
    "                W[i].append([])\n",
    "                W[i][j] = [random.uniform(-1,1) for j in range(len(W[i-1]))]\n",
    "                b[i].append(0)\n",
    "                #se crea un bias aleatorio para la neurona\n",
    "                b[i][j] = random.uniform(-1,1) \n",
    "            \n",
    "                W[i][j] = np.array([W[i][j]])\n",
    "                \n",
    "        \n",
    "        #Ahora se insertan los pesos y bias al output unit\n",
    "        W.append([])\n",
    "        W[len(W) - 1].append([])\n",
    "        b.append([])\n",
    "        b[len(W) - 1].append(0)\n",
    "        W[len(W) - 1][0] = [random.uniform(-1,1) for j in range(len(W[len(W)-2]))]\n",
    "        W[len(W) - 1][0] = np.array([W[len(W) - 1][0]])\n",
    "        b[len(W) - 1][0] = random.uniform(-1,1)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"Network Built\")\n",
    "            print(\"W\")\n",
    "            print(W)\n",
    "            print(\"b\")\n",
    "            print(b)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "            \n",
    "    #X es una entrada\n",
    "    #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "    #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "    #l será el número de capas.\n",
    "    #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "    #de la capa 0.\n",
    "    #Algoritmo se basa en el algoritmo 6.3 de GoodFellow et al \n",
    "    #y en el algoritmo de la página 217 de Grus en Data Science from Scratch\n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        l = len(self.hidden_layer)\n",
    "        \n",
    "        #Salidas de cada neurona. Primera dimensión capa, segunda neuronas\n",
    "        #outputs[0][0] representa la entrada 0 de X. \n",
    "        #outputs[1][0] representa la salida 0 de la primera capa escondida.\n",
    "        outputs = [np.array([x])]\n",
    "        A = []\n",
    "        \n",
    "        for layer in range(1,l+1):\n",
    "            \n",
    "            layer_out = []\n",
    "            A_out = []\n",
    "            for neuron in range(len(self.W[layer-1])):\n",
    "                #a es el resultado de las salidas de la capa anterior por los pesos de la capa actual.\n",
    "                \n",
    "                X = outputs[layer-1].transpose()\n",
    "                W = self.W[layer-1][neuron]\n",
    "                a = np.dot(W,X)[0][0] + self.b[layer-1][neuron]\n",
    "                layer_out.append(self.relu(a))\n",
    "                \n",
    "            outputs.append(np.array([layer_out]))\n",
    "            \n",
    "        #Ahora se computa el output del output layer.\n",
    "        X = outputs[len(self.W)-1].transpose()\n",
    "        W = self.W[len(self.W)-1][0]\n",
    "        np.dot(W,X)[0][0]\n",
    "        self.b[len(self.W)-1][0]\n",
    "        a = np.dot(W,X)[0][0] + self.b[len(self.W)-1][0]\n",
    "        outputs.append(self.sigmoid(a))\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"----------------------Feed_forward outputs--------------------------\")\n",
    "            print(outputs)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        #Retorna el sigmoide del último output. Esto representa la probabilidad de que \n",
    "        #x pertenezca a la clase 1.\n",
    "        self.outputs = outputs\n",
    "        self.A = A\n",
    "        return outputs[len(outputs)-1]\n",
    "    \n",
    "    #Relu para hidden units\n",
    "    def relu(self, z):\n",
    "        return max(0,z)\n",
    "    \n",
    "    #Sigmoide para Output unit\n",
    "    def sigmoid(self, x):\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1+math.exp(x))\n",
    "        else:\n",
    "            return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def relu_derivative(self, z): \n",
    "        if z > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s*(1-s)\n",
    "    \n",
    "    #sea gradiente de la salida.\n",
    "    def back_propagation(self, g, learning_rate):\n",
    "        \n",
    "        W = self.W\n",
    " \n",
    "        #Calcular pesos de la capa de salida, se hace por aparte porque \n",
    "        #Usa función de activación sigmoide en lugar de relu.\n",
    "    \n",
    "        a = self.outputs[len(self.outputs)-1]\n",
    "        \n",
    "        f_a_prime = self.sigmoid_derivative(a)\n",
    "        \n",
    "        g = g*f_a_prime\n",
    "        \n",
    "        h = self.outputs[len(self.outputs)-2]\n",
    "        \n",
    "        w_grad = np.dot(g, h)\n",
    "        b_grad = g\n",
    "        \n",
    "        W_k  = W[len(W) - 1][0]\n",
    "        \n",
    "        g = np.dot(W_k.transpose(), g)\n",
    "        \n",
    "        for i in range(len(self.outputs)-2, 0, -1):\n",
    "            a = self.outputs[i][0]\n",
    "            \n",
    "            f_a_prime = [self.sigmoid_derivative(j) for j in a]\n",
    "            f_a_prime = np.array([f_a_prime]).transpose()\n",
    "            \n",
    "            g = np.multiply(g, f_a_prime)\n",
    "            \n",
    "            h = self.outputs[i-1]\n",
    "        \n",
    "            w_grad = np.dot(g, h)\n",
    "            b_grad = g\n",
    "        \n",
    "            W_k  = W[i-1]    \n",
    "            W_k = self.listmatrix_to_matrix(W_k)\n",
    "            g = np.dot(W_k.transpose(), g)\n",
    "\n",
    "        self.W = W\n",
    "        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def listmatrix_to_matrix(self, matlist):\n",
    "        return np.array([i[0] for i in matlist])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 8330\n",
      "Columnas de la matriz: 31\n",
      "[[-18.996       89.147        1.         ... 233.20616681 261.85070337\n",
      "  240.83417734]\n",
      " [-19.347      125.825        4.         ... 227.2751235  261.64357048\n",
      "  332.35653566]\n",
      " [ -9.472      121.707        4.         ... 549.49321044 481.14904868\n",
      "  442.66313626]\n",
      " ...\n",
      " [ -9.494       88.976        4.         ... 519.52773394 538.89585608\n",
      "  313.77593362]\n",
      " [ -7.617       67.929        3.         ... 591.85304812 598.05409088\n",
      "  443.89380682]\n",
      " [-11.774       85.176        3.         ... 659.32142175 531.85019809\n",
      "  607.21596134]]\n",
      "[ 1  1  1 ... -1 -1 -1]\n",
      "X shape(8330, 30)\n",
      "y shape(8330,)\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./msd_genre_dataset/fixed_ds.csv\", \"r\"), delimiter=\",\", skiprows=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "X = np.delete(data_matrix, len(data_matrix[0])-1,1)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "#Se intenta estandarizar X para lograr mejor desempeño. Sin embargo, no parece funcionar.\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#Los datos del set de datos están agrupados por género. Es decir, primero están todas las filas que corresponden\n",
    "#a 1 y después todas las que corresponden a -1. Se hace un shuffle para que, más tarde, en cross-validation\n",
    "#no se creen unos modelos que predigan únicamente una clase.\n",
    "np.random.shuffle(data_matrix)\n",
    "\n",
    "\n",
    "print(\"X shape\" + str(X.shape))\n",
    "print(\"y shape\" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Built\n",
      "W\n",
      "[[array([[-0.13932121,  0.26154431,  0.12722945, -0.59760968, -0.45392351,\n",
      "        -0.22648387, -0.569504  , -0.50688154,  0.91308589,  0.48755504,\n",
      "        -0.69997705,  0.75622059, -0.01906391, -0.80831366,  0.45441617,\n",
      "        -0.54627429, -0.70085863, -0.05527424, -0.77410725, -0.65273755,\n",
      "        -0.09458312,  0.14185284, -0.5499195 ,  0.92470709,  0.93921082,\n",
      "         0.65276327,  0.71051165, -0.04089216, -0.92819459,  0.8771099 ]]), array([[ 0.07752813, -0.634814  , -0.79256433, -0.33462951, -0.33023806,\n",
      "        -0.93181233, -0.03070471, -0.49532082,  0.6379007 ,  0.64615244,\n",
      "         0.59709062,  0.92635479,  0.69017909,  0.62300727, -0.12154857,\n",
      "         0.24245174,  0.44962666, -0.18395253, -0.7469736 , -0.7324934 ,\n",
      "        -0.30616671,  0.76571855,  0.80568771,  0.39561133,  0.98820895,\n",
      "        -0.82740776,  0.73131489, -0.40171549, -0.70206383, -0.38740353]]), array([[ 0.18434297, -0.10551313, -0.73867932,  0.42681321, -0.842221  ,\n",
      "         0.19534009, -0.06882088,  0.90116193,  0.77871143,  0.3775198 ,\n",
      "        -0.12323527,  0.53553881,  0.40738413, -0.02153889,  0.28909487,\n",
      "         0.95529581, -0.15440694, -0.68807566,  0.42689748, -0.37911738,\n",
      "        -0.17088231, -0.36307637,  0.02396137, -0.11004369, -0.01842103,\n",
      "         0.36350983,  0.74346859, -0.02534072,  0.50534805,  0.50126989]])], [array([[ 0.08335562,  0.16673381, -0.0874336 ]]), array([[0.57005329, 0.49651077, 0.82063052]])], [array([[-0.15444553,  0.24887373]]), array([[-0.46087094, -0.2663034 ]]), array([[ 0.38926791, -0.91200124]])], [array([[-0.60779524, -0.75285413,  0.43050892]])]]\n",
      "b\n",
      "[[-0.18983772283958822, -0.5842896705922274, 0.39310489820573813], [0.1587927860444378, 0.29959996041580417], [0.1681530368980264, 0.384864328959047, 0.5004863984336483], [0.6744746813721205]]\n",
      "--------------------------------------------------------------------\n",
      "----------------------Feed_forward outputs--------------------------\n",
      "[array([[0.43689892, 0.38534851, 0.57142857, 0.09090909, 1.        ,\n",
      "        0.01256741, 0.42638424, 0.50882769, 0.25229911, 0.40404321,\n",
      "        0.01836715, 0.28985667, 0.50330378, 0.56040409, 0.71819616,\n",
      "        0.67856873, 0.57828256, 0.4646388 , 0.3325513 , 0.09528135,\n",
      "        0.08016889, 0.05397623, 0.26086699, 0.13086394, 0.05279261,\n",
      "        0.06285587, 0.21449071, 0.03928601, 0.11222406, 0.08747566]]), array([[0.        , 0.        , 1.12621315]]), array([[0.06032392, 1.22380484]]), array([[0.46340916, 0.0311594 , 0.        ]]), 0.5913039160473111]\n",
      "--------------------------------------------------------------------\n",
      "FF: 0.5913039160473111 \n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptronClassifier(hidden_layer = [3,2,3], suppress_output = False)\n",
    "\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
