{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user numpy\n",
    "%pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClassifier:\n",
    "    \n",
    "    def __init__(self, hidden_layer, suppress_output = False):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.suppress_output = False\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.outputs = None\n",
    "        self.A = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        #Se hará la suposición que todas las filas de X tienen el mismo número de columnas.\n",
    "        self.build_network(len(X[0]))\n",
    "        \n",
    "        index = math.floor(random.random()*len(X))\n",
    "        \n",
    "        y_pred = self.feed_forward(X[index])\n",
    "        print(\"FF: %s \" % y_pred)\n",
    "        \n",
    "        self.back_propagation(X[index], y[index], y_pred, learning_rate=0.01)\n",
    "\n",
    "    \n",
    "    #input_length esero entero positivo mayor a 1 que representa la cantidad de atributos de cada\n",
    "    #dato de entrada (sin incluir el que se busca predecir).\n",
    "    #hidden_dimensions es una lista que contiene las dimensiones de las capas escondidas de la red\n",
    "    #si hidden_dimensions es [3,3,3] se crearán tres capas escondidas de \n",
    "    def build_network(self, input_length):\n",
    "        \n",
    "        dimensions = self.hidden_layer                   \n",
    "        \n",
    "        #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "        #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "        W = []  \n",
    "        for i in range(len(dimensions)): \n",
    "            W.append([])\n",
    "        \n",
    "        \n",
    "        #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "        #de la capa 0\n",
    "        b = []\n",
    "        for i in range(len(dimensions)): \n",
    "            b.append([])\n",
    "        \n",
    "        \n",
    "        #todas las neuronas en la primera capa deben tener input_lenght pesos. De ahí en adelante, \n",
    "        #cada neurona en la capa j tendrá len(W[j-1]) pesos.\n",
    "        layer_width = dimensions[0]\n",
    "        \n",
    "        for i in range(layer_width):\n",
    "            W[0].append([])\n",
    "            W[0][i] = [random.uniform(-1,1) for j in range(input_length)]\n",
    "            b[0].append(0)\n",
    "            b[0][i] = random.uniform(-1,1)\n",
    "            W[0][i] = np.array([W[0][i]])\n",
    "            \n",
    "\n",
    "            \n",
    "        #Inicialización aleatoria de pesos y bias entre -1 y 1\n",
    "        for i in range(1, len(dimensions)):\n",
    "            layer_width = dimensions[i]\n",
    "            for j in range(layer_width):\n",
    "                #se inicializan n valores aleatorios entre -1 y 1 donde n = len(W[i-1])\n",
    "                W[i].append([])\n",
    "                W[i][j] = [random.uniform(-1,1) for j in range(len(W[i-1]))]\n",
    "                b[i].append(0)\n",
    "                #se crea un bias aleatorio para la neurona\n",
    "                b[i][j] = random.uniform(-1,1) \n",
    "            \n",
    "                W[i][j] = np.array([W[i][j]])\n",
    "                \n",
    "        \n",
    "        #Ahora se insertan los pesos y bias al output unit\n",
    "        W.append([])\n",
    "        W[len(W) - 1].append([])\n",
    "        b.append([])\n",
    "        b[len(W) - 1].append(0)\n",
    "        W[len(W) - 1][0] = [random.uniform(-1,1) for j in range(len(W[len(W)-2]))]\n",
    "        W[len(W) - 1][0] = np.array([W[len(W) - 1][0]])\n",
    "        b[len(W) - 1][0] = random.uniform(-1,1)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"Network Built\")\n",
    "            print(\"W\")\n",
    "            print(W)\n",
    "            print(\"b\")\n",
    "            print(b)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "            \n",
    "    #X es una entrada\n",
    "    #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "    #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "    #l será el número de capas.\n",
    "    #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "    #de la capa 0.\n",
    "    #Algoritmo se basa en el algoritmo 6.3 de GoodFellow et al \n",
    "    #y en el algoritmo de la página 217 de Grus en Data Science from Scratch\n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        l = len(self.hidden_layer)\n",
    "        \n",
    "        #Salidas de cada neurona. Primera dimensión capa, segunda neuronas\n",
    "        #outputs[0][0] representa la entrada 0 de X. \n",
    "        #outputs[1][0] representa la salida 0 de la primera capa escondida.\n",
    "        outputs = [np.array([x])]\n",
    "        A = []\n",
    "        \n",
    "        for layer in range(1,l):\n",
    "            \n",
    "            layer_out = []\n",
    "            A_out = []\n",
    "            for neuron in range(len(self.W[layer])):\n",
    "                #a es el resultado de las salidas de la capa anterior por los pesos de la capa actual.\n",
    "                \n",
    "                X = outputs[layer-1].transpose()\n",
    "                W = self.W[layer-1][neuron]\n",
    "                \n",
    "                a = np.dot(W,X)[0][0] + self.b[layer][neuron]\n",
    "                layer_out.append(self.relu(a))\n",
    "                A_out.append(a)\n",
    "                \n",
    "            outputs.append(np.array([layer_out]))\n",
    "            A.append(np.array([A_out]))\n",
    "            \n",
    "        #Ahora se computa el output del output layer. \n",
    "        X = outputs[len(self.W)-2].transpose()\n",
    "        W = self.W[len(self.W)-1][0]\n",
    "        np.dot(W,X)[0][0]\n",
    "        self.b[len(self.W)-1][0]\n",
    "        a = np.dot(W,X)[0][0] + self.b[len(self.W)-1][0]\n",
    "        outputs.append(self.sigmoid(a))\n",
    "        A.append(a)\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"----------------------Feed_forward outputs--------------------------\")\n",
    "            print(outputs)\n",
    "            print(\"----------------------Feed_forward A's------------------------------\")\n",
    "            print(A)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        #Retorna el sigmoide del último output. Esto representa la probabilidad de que \n",
    "        #x pertenezca a la clase 1.\n",
    "        self.outputs = outputs\n",
    "        self.A = A\n",
    "        return outputs[len(outputs)-1]\n",
    "    \n",
    "    #Relu para hidden units\n",
    "    def relu(self, z):\n",
    "        return max(0,z)\n",
    "    \n",
    "    #Sigmoide para Output unit\n",
    "    def sigmoid(self, x):\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1+math.exp(x))\n",
    "        else:\n",
    "            return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def relu_derivative(self, z): \n",
    "        if z > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s*(1-s)\n",
    "    \n",
    "    #Sean X e y los datos escogidos para realizar la estimación en línea del gradiente.\n",
    "    def back_propagation(self, x, y, y_pred, learning_rate):\n",
    "        #Activación traspuesta producto raro W transpuesta producto error del siguiente. \n",
    "        #Matriz bidimensional que contiene los errores de las capas escondidas. \n",
    "        \n",
    "        #Los pesos del output se ajustan, usando descenso de gradiente, con base a e\n",
    "        W = self.W\n",
    "       \n",
    "        #TODO, no estoy seguro si se usa este error o el error del perceptrón\n",
    "        #e = y - y_pred\n",
    "        #estimated_grad = np.multiply(e, x)\n",
    "        #\n",
    "        #W[len(W) - 1] = W[len(W) - 1] + np.multiply(learning_rate, estimated_grad)\n",
    "        \n",
    "        #Compute the gradient of the output layer\n",
    "        #en linea, se utiliza únicamente la pareja (x,y) para estimar.\n",
    "        g = y - y_pred\n",
    "        delta_out = np.multiply(g, self.sigmoid(y_pred))\n",
    "                \n",
    "        delta = []\n",
    "        #deltas de las capas escondidas + out layer\n",
    "        for i in range(len(self.hidden_layer)+1):\n",
    "            delta.append(np.array([]))\n",
    "        \n",
    "        delta[len(self.hidden_layer)] = delta_out\n",
    "          \n",
    "        #print(len(self.outputs))\n",
    "        #print(len(self.hidden_layer))\n",
    "        for i in range(len(self.hidden_layer)-1, -1, -1):\n",
    "            #Calcular f' de las activaciones\n",
    "            a = self.outputs[i][0]\n",
    "            #f' de los outputs\n",
    "            f_a_prime = [self.relu_derivative(i) for i in a]\n",
    "            f_a_prime = np.array([f_a_prime]).transpose()\n",
    "            \n",
    "            \n",
    "            print(\"f_a_prime\")\n",
    "            print(f_a_prime)\n",
    "            \n",
    "            g = np.multiply(g, f_a_prime)\n",
    "        \n",
    "            \n",
    "            h = self.outputs[i-1]\n",
    "            \n",
    "            print(\"g\")\n",
    "            print(g)\n",
    "            print(\"h\")\n",
    "            print(h)\n",
    "            \n",
    "            w_grad = np.dot(g, h)\n",
    "            b_grad = g\n",
    "            \n",
    "            print(\"w_grad, b_grad\")\n",
    "            print(w_grad)\n",
    "            print(b_grad)\n",
    "            \n",
    "            #g = (W_t^T)g\n",
    "            print(\"W_k\")\n",
    "            print(W[i])\n",
    "\n",
    "            g = np.dot(W[i].transpose(), g)\n",
    "            print(\"new g\")\n",
    "            print(g)\n",
    "            \n",
    "        #for i in range(len(self.hidden_layer)):\n",
    "        #    delta.append([])\n",
    "        \n",
    "        #Calcular error de la última capa escondida\n",
    "        #(caso especial)\n",
    "        \n",
    "        \n",
    "        self.W = W\n",
    "        \n",
    "        #Los pesos de acá multiplicado por el error de la siguiente capa\n",
    "        #np.dot(W[i+1].transpose(), e)\n",
    "        \n",
    "        #np.multiply(outputs[i].transpose(), )\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 8330\n",
      "Columnas de la matriz: 31\n",
      "[[-18.996       89.147        1.         ... 233.20616681 261.85070337\n",
      "  240.83417734]\n",
      " [-19.347      125.825        4.         ... 227.2751235  261.64357048\n",
      "  332.35653566]\n",
      " [ -9.472      121.707        4.         ... 549.49321044 481.14904868\n",
      "  442.66313626]\n",
      " ...\n",
      " [ -9.494       88.976        4.         ... 519.52773394 538.89585608\n",
      "  313.77593362]\n",
      " [ -7.617       67.929        3.         ... 591.85304812 598.05409088\n",
      "  443.89380682]\n",
      " [-11.774       85.176        3.         ... 659.32142175 531.85019809\n",
      "  607.21596134]]\n",
      "[ 1  1  1 ... -1 -1 -1]\n",
      "X shape(8330, 30)\n",
      "y shape(8330,)\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./msd_genre_dataset/fixed_ds.csv\", \"r\"), delimiter=\",\", skiprows=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "X = np.delete(data_matrix, len(data_matrix[0])-1,1)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "#Se intenta estandarizar X para lograr mejor desempeño. Sin embargo, no parece funcionar.\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#Los datos del set de datos están agrupados por género. Es decir, primero están todas las filas que corresponden\n",
    "#a 1 y después todas las que corresponden a -1. Se hace un shuffle para que, más tarde, en cross-validation\n",
    "#no se creen unos modelos que predigan únicamente una clase.\n",
    "np.random.shuffle(data_matrix)\n",
    "\n",
    "\n",
    "print(\"X shape\" + str(X.shape))\n",
    "print(\"y shape\" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Built\n",
      "W\n",
      "[[array([[ 0.33689729,  0.29313145, -0.90371925, -0.58911444,  0.29916219,\n",
      "         0.91794687, -0.39709357,  0.87340645,  0.15178005,  0.7773885 ,\n",
      "        -0.49769426,  0.19100682, -0.86014261, -0.40219076,  0.37868386,\n",
      "        -0.09595167,  0.68328883,  0.53901887,  0.39866176,  0.35978874,\n",
      "         0.83656233,  0.08068399, -0.69733708,  0.12641612,  0.56926875,\n",
      "         0.74584889, -0.29744623,  0.31011283,  0.24959301, -0.55466537]]), array([[-0.29802593, -0.14709702, -0.96916042, -0.89725089, -0.00132237,\n",
      "         0.14512391,  0.30797551, -0.14070085,  0.86141915, -0.31712035,\n",
      "        -0.90893442, -0.60702624, -0.20808579,  0.07650591, -0.40341241,\n",
      "         0.1802478 , -0.70916021,  0.41703984, -0.54069441, -0.44703756,\n",
      "         0.01845403, -0.64091321,  0.12624951, -0.32155099,  0.75745946,\n",
      "         0.46582408, -0.98438244,  0.74805914,  0.79402735,  0.18365923]]), array([[ 0.65026178,  0.58659524, -0.19044691,  0.86201524,  0.94326603,\n",
      "         0.08993959, -0.86822732, -0.92382225,  0.50995091,  0.82261875,\n",
      "        -0.04885749,  0.85839648, -0.42088948, -0.45402444, -0.55453377,\n",
      "        -0.52221006, -0.24283192, -0.48335282, -0.72243392,  0.35497308,\n",
      "        -0.5998967 , -0.95144419,  0.74813414, -0.83985493,  0.77034853,\n",
      "        -0.57579495,  0.62494794,  0.36643081, -0.86227464, -0.48872499]])], [array([[-0.2506718 , -0.01645908,  0.41302939]]), array([[0.61025742, 0.86105526, 0.42187213]]), array([[-0.60905794, -0.23627662,  0.10621135]])], [array([[-0.38346424, -0.58304331, -0.18723031]]), array([[-0.92195161,  0.44333941,  0.01502338]]), array([[0.28899548, 0.34853806, 0.40204657]])], [array([[ 0.9887347 , -0.81169561,  0.33821894]])]]\n",
      "b\n",
      "[[0.5477803976420481, -0.37081714126130794, -0.2935336557083972], [0.13133475747376955, -0.7831899014892547, 0.6993041983504804], [0.4096877562632424, -0.1860411799353494, -0.11152617860885972], [0.6809944928359675]]\n",
      "--------------------------------------------------------------------\n",
      "----------------------Feed_forward outputs--------------------------\n",
      "[array([[0.62590775, 0.33886417, 1.        , 0.81818182, 0.        ,\n",
      "        0.10775051, 0.64323346, 0.31889798, 0.45143795, 0.48995143,\n",
      "        0.4979915 , 0.27761326, 0.53953604, 0.46117875, 0.63775393,\n",
      "        0.42515074, 0.73842198, 0.54146187, 0.09367533, 0.04570868,\n",
      "        0.07582361, 0.0722565 , 0.08900795, 0.07534017, 0.08672603,\n",
      "        0.08320711, 0.124246  , 0.12500974, 0.0814849 , 0.12749396]]), array([[0.        , 0.        , 0.21800027]]), array([[0.49972828, 0.        , 0.        ]]), 0.764064397635732]\n",
      "----------------------Feed_forward A's------------------------------\n",
      "[array([[-0.08665885, -3.46232171,  0.21800027]]), array([[ 0.49972828, -0.09407294, -0.08837208]]), 1.17509317926183]\n",
      "--------------------------------------------------------------------\n",
      "FF: 0.764064397635732 \n",
      "f_a_prime\n",
      "[[1]\n",
      " [0]\n",
      " [0]]\n",
      "g\n",
      "[[-1.7640644]\n",
      " [-0.       ]\n",
      " [-0.       ]]\n",
      "h\n",
      "[[0.         0.         0.21800027]]\n",
      "w_grad, b_grad\n",
      "[[-0.         -0.         -0.38456652]\n",
      " [-0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.        ]]\n",
      "[[-1.7640644]\n",
      " [-0.       ]\n",
      " [-0.       ]]\n",
      "W_k\n",
      "[array([[-0.38346424, -0.58304331, -0.18723031]]), array([[-0.92195161,  0.44333941,  0.01502338]]), array([[0.28899548, 0.34853806, 0.40204657]])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-2b517ff6fb50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLayerPerceptronClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-392-8b269a66e04e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FF: %s \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-392-8b269a66e04e>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(self, x, y, y_pred, learning_rate)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptronClassifier(hidden_layer = [3, 3, 3], suppress_output = False)\n",
    "\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
