{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user numpy\n",
    "%pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClassifier:\n",
    "    \n",
    "    def __init__(self, hidden_layer, suppress_output = False):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.suppress_output = False\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.outputs = None\n",
    "        self.A = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        #Se hará la suposición que todas las filas de X tienen el mismo número de columnas.\n",
    "        self.build_network(len(X[0]))\n",
    "        \n",
    "        print(\"FF: %s \" % self.feed_forward(X[math.floor(random.random()*len(X))]))\n",
    "\n",
    "    \n",
    "    #input_length esero entero positivo mayor a 1 que representa la cantidad de atributos de cada\n",
    "    #dato de entrada (sin incluir el que se busca predecir).\n",
    "    #hidden_dimensions es una lista que contiene las dimensiones de las capas escondidas de la red\n",
    "    #si hidden_dimensions es [3,3,3] se crearán tres capas escondidas de \n",
    "    def build_network(self, input_length):\n",
    "        \n",
    "        dimensions = self.hidden_layer                   \n",
    "        \n",
    "        #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "        #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "        W = []  \n",
    "        for i in range(len(dimensions)): \n",
    "            W.append([])\n",
    "        \n",
    "        \n",
    "        #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "        #de la capa 0\n",
    "        b = []\n",
    "        for i in range(len(dimensions)): \n",
    "            b.append([])\n",
    "        \n",
    "        \n",
    "        #todas las neuronas en la primera capa deben tener input_lenght pesos. De ahí en adelante, \n",
    "        #cada neurona en la capa j tendrá len(W[j-1]) pesos.\n",
    "        layer_width = dimensions[0]\n",
    "        \n",
    "        for i in range(layer_width):\n",
    "            W[0].append([])\n",
    "            W[0][i] = [random.uniform(-1,1) for j in range(input_length)]\n",
    "            b[0].append(0)\n",
    "            b[0][i] = random.uniform(-1,1)\n",
    "            W[0][i] = np.array([W[0][i]])\n",
    "            \n",
    "\n",
    "            \n",
    "        #Inicialización aleatoria de pesos y bias entre -1 y 1\n",
    "        for i in range(1, len(dimensions)):\n",
    "            layer_width = dimensions[i]\n",
    "            for j in range(layer_width):\n",
    "                #se inicializan n valores aleatorios entre -1 y 1 donde n = len(W[i-1])\n",
    "                W[i].append([])\n",
    "                W[i][j] = [random.uniform(-1,1) for j in range(len(W[i-1]))]\n",
    "                b[i].append(0)\n",
    "                #se crea un bias aleatorio para la neurona\n",
    "                b[i][j] = random.uniform(-1,1) \n",
    "            \n",
    "                W[i][j] = np.array([W[i][j]])\n",
    "                \n",
    "        \n",
    "        #Ahora se insertan los pesos y bias al output unit\n",
    "        W.append([])\n",
    "        W[len(W) - 1].append([])\n",
    "        b.append([])\n",
    "        b[len(W) - 1].append(0)\n",
    "        W[len(W) - 1][0] = [random.uniform(-1,1) for j in range(len(W[len(W)-2]))]\n",
    "        W[len(W) - 1][0] = np.array([W[len(W) - 1][0]])\n",
    "        b[len(W) - 1][0] = random.uniform(-1,1)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"Network Built\")\n",
    "            print(\"W\")\n",
    "            print(W)\n",
    "            print(\"b\")\n",
    "            print(b)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "            \n",
    "    #X es una entrada\n",
    "    #W matriz de pesos. 3D, primera dimensión capa, Segunda dimensión neurona, tercera dimensión pesos. \n",
    "    #Entonces, W[0][0] contendría el arreglo con los pesos de la neurona 0 de la capa 0.\n",
    "    #l será el número de capas.\n",
    "    #b es un arreglo 2D que contiene el bias de cada neurona b[0][0] contiene el bias de la neurona 0 \n",
    "    #de la capa 0.\n",
    "    #Algoritmo se basa en el algoritmo 6.3 de GoodFellow et al \n",
    "    #y en el algoritmo de la página 217 de Grus en Data Science from Scratch\n",
    "    def feed_forward(self, x):\n",
    "        \n",
    "        l = len(self.hidden_layer)\n",
    "        \n",
    "        #Salidas de cada neurona. Primera dimensión capa, segunda neuronas\n",
    "        #outputs[0][0] representa la entrada 0 de X. \n",
    "        #outputs[1][0] representa la salida 0 de la primera capa escondida.\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = [np.array([x])]\n",
    "        A = []\n",
    "        for layer in range(1,l):\n",
    "            \n",
    "            layer_out = []\n",
    "            A_out = []\n",
    "            for neuron in range(len(self.W[layer])):\n",
    "                #a es el resultado de las salidas de la capa anterior por los pesos de la capa actual.\n",
    "                \n",
    "                X = outputs[layer-1].transpose()\n",
    "                W = self.W[layer-1][neuron]\n",
    "                \n",
    "                a = np.dot(W,X)[0][0] + self.b[layer][neuron]\n",
    "                layer_out.append(self.relu(a))\n",
    "                A_out.append(a)\n",
    "                \n",
    "            outputs.append(np.array([layer_out]))\n",
    "            A.append(np.array([A_out]))\n",
    "            \n",
    "        #Ahora se computa el output del output layer. \n",
    "        X = outputs[len(self.W)-2].transpose()\n",
    "        W = self.W[len(self.W)-1][0]\n",
    "        print(\"W AND X\")\n",
    "        print(W)\n",
    "        print(X)\n",
    "        np.dot(W,X)[0][0]\n",
    "        self.b[len(self.W)-1][0]\n",
    "        a = np.dot(W,X)[0][0] + self.b[len(self.W)-1][0]\n",
    "        outputs.append(self.sigmoid(a))\n",
    "        A.append(a)\n",
    "        \n",
    "        if not self.suppress_output:\n",
    "            print(\"----------------------Feed_forward outputs--------------------------\")\n",
    "            print(outputs)\n",
    "            print(\"----------------------Feed_forward A's------------------------------\")\n",
    "            print(A)\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        #Retorna el sigmoide del último output. Esto representa la probabilidad de que \n",
    "        #x pertenezca a la clase 1.\n",
    "        self.outputs = outputs\n",
    "        self.A = A\n",
    "        return outputs[len(outputs)-1]\n",
    "    \n",
    "    #Relu para hidden units\n",
    "    def relu(self, z):\n",
    "        return max(0,z)\n",
    "    \n",
    "    #Sigmoide para Output unit\n",
    "    def sigmoid(self, x):\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1+math.exp(x))\n",
    "        else:\n",
    "            return 1/(1+math.exp(-x))\n",
    "    \n",
    "    def relu_derivative(self, z): \n",
    "        if z > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def back_propagation(self, e):\n",
    "        #Activación traspuesta producto raro W transpuesta producto error del siguiente. \n",
    "        #Matriz bidimensional que contiene los errores de las capas escondidas. \n",
    "        #Primera dimensión capa, segunda neurona.\n",
    "        \n",
    "        delta = []\n",
    "        \n",
    "        for i in range(len(self.hidden_layer)):\n",
    "            \n",
    "        \n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 8330\n",
      "Columnas de la matriz: 31\n",
      "[[-18.996       89.147        1.         ... 233.20616681 261.85070337\n",
      "  240.83417734]\n",
      " [-19.347      125.825        4.         ... 227.2751235  261.64357048\n",
      "  332.35653566]\n",
      " [ -9.472      121.707        4.         ... 549.49321044 481.14904868\n",
      "  442.66313626]\n",
      " ...\n",
      " [ -9.494       88.976        4.         ... 519.52773394 538.89585608\n",
      "  313.77593362]\n",
      " [ -7.617       67.929        3.         ... 591.85304812 598.05409088\n",
      "  443.89380682]\n",
      " [-11.774       85.176        3.         ... 659.32142175 531.85019809\n",
      "  607.21596134]]\n",
      "[ 1  1  1 ... -1 -1 -1]\n",
      "X shape(8330, 30)\n",
      "y shape(8330,)\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./msd_genre_dataset/fixed_ds.csv\", \"r\"), delimiter=\",\", skiprows=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "X = np.delete(data_matrix, len(data_matrix[0])-1,1)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "#Se intenta estandarizar X para lograr mejor desempeño. Sin embargo, no parece funcionar.\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#Los datos del set de datos están agrupados por género. Es decir, primero están todas las filas que corresponden\n",
    "#a 1 y después todas las que corresponden a -1. Se hace un shuffle para que, más tarde, en cross-validation\n",
    "#no se creen unos modelos que predigan únicamente una clase.\n",
    "np.random.shuffle(data_matrix)\n",
    "\n",
    "\n",
    "print(\"X shape\" + str(X.shape))\n",
    "print(\"y shape\" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Built\n",
      "W\n",
      "[[array([[ 0.20404593, -0.9418346 , -0.78360462, -0.10668519,  0.4818155 ,\n",
      "         0.66538047,  0.71790641,  0.30737451, -0.17846904, -0.84924797,\n",
      "        -0.50004533, -0.4560268 ,  0.14124846,  0.65037077, -0.92330598,\n",
      "        -0.08337044, -0.00682228, -0.50977811, -0.41016895,  0.05702923,\n",
      "        -0.09608899,  0.24522899, -0.21826192,  0.62454997, -0.10225758,\n",
      "        -0.59927718,  0.36533433,  0.78862051,  0.2545183 ,  0.80249054]]), array([[-0.84850357,  0.04744941,  0.21744366,  0.66675071,  0.44971991,\n",
      "        -0.93947107, -0.1160988 , -0.04195025, -0.47762897,  0.04926024,\n",
      "        -0.45527995, -0.78896355, -0.49554908,  0.76925949,  0.34440754,\n",
      "        -0.0533385 ,  0.24558756, -0.62867312,  0.163873  , -0.03228492,\n",
      "        -0.43995764, -0.99369471, -0.64750272, -0.4290368 ,  0.22823547,\n",
      "        -0.63329734,  0.06641409,  0.62022267,  0.91807511,  0.8571302 ]]), array([[-0.55319969, -0.63143133, -0.57615056, -0.16564682,  0.26162562,\n",
      "         0.01608506,  0.11398343, -0.055815  ,  0.66244286, -0.18007052,\n",
      "         0.28870719,  0.4954443 , -0.66682323,  0.71443497, -0.01701605,\n",
      "         0.01495767, -0.64599405,  0.36712456,  0.6299601 ,  0.41511595,\n",
      "         0.06387939, -0.72300396, -0.01147095,  0.28227074, -0.52166305,\n",
      "        -0.70135079, -0.24328618,  0.29586537,  0.49237403, -0.37109082]])], [array([[-0.31520898,  0.25545548, -0.93460942]]), array([[ 0.34219548, -0.07474725,  0.87265808]]), array([[ 0.06839479, -0.14495441,  0.07127913]])], [array([[ 0.27917345, -0.56127056, -0.87053583]]), array([[-0.71383212,  0.68664265,  0.08497176]]), array([[-0.54942496,  0.41590632,  0.77386675]])], [array([[-0.94111789, -0.73127381, -0.7781721 ]])]]\n",
      "b\n",
      "[[-0.39733509764664277, 0.3904400352274535, 0.36252146489555015], [0.4524590768340373, -0.22285383128791403, 0.2504726551237928], [-0.34936859772345175, -0.7214837510080003, 0.1343817731534298], [0.6581914311567516]]\n",
      "--------------------------------------------------------------------\n",
      "W AND X\n",
      "[[-0.94111789 -0.73127381 -0.7781721 ]]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.13438177]]\n",
      "----------------------Feed_forward outputs--------------------------\n",
      "[array([[0.55851325, 0.45718724, 0.57142857, 0.63636364, 1.        ,\n",
      "        0.14618022, 0.55131755, 0.29708469, 0.21932415, 0.35806726,\n",
      "        0.4704839 , 0.22949864, 0.45281719, 0.45756092, 0.66389035,\n",
      "        0.47338947, 0.70347743, 0.48362164, 0.10665671, 0.11882853,\n",
      "        0.07163346, 0.06069701, 0.16087253, 0.07878223, 0.08067048,\n",
      "        0.11036001, 0.13481868, 0.25534089, 0.06545133, 0.10548612]]), array([[0, 0, 0]]), array([[0.        , 0.        , 0.13438177]]), 0.6349748844633599]\n",
      "----------------------Feed_forward A's------------------------------\n",
      "[array([[-0.27636556, -0.12307092, -0.38344934]]), array([[-0.3493686 , -0.72148375,  0.13438177]]), 0.5536192851796463]\n",
      "--------------------------------------------------------------------\n",
      "FF: 0.6349748844633599 \n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptronClassifier(hidden_layer = [3, 3, 3], suppress_output = False)\n",
    "\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
